{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "За класифициране на изображения, както е известно, най-добри резултати дава CNN (Convolutional Neural Network) и затова съм избрала да разгледам точно CNN. Малко по-надолу в тетрадката съм представила няколко варианта на CNN:\n",
    "    - CNN, която съм натренирала за първите пет породи в данните\n",
    "    - CNN, която съм натренирала за първите десет породи в данните\n",
    "    - CNN, която класифицира всичките 133 породи, но за да успея да постигна това, и то с добър резултат, съм използвала метода Transfer Learning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Z20KM61qysre"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "import keras.utils as image\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from PIL import ImageFile\n",
    "from sklearn.datasets import load_files\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('data/train/train.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('train_5')\n",
    "with zipfile.ZipFile('data/test/test.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('test_5')\n",
    "with zipfile.ZipFile('data/valid/valid.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('valid_5')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Горния код служи, както може да се подразбере, за разархивиране на данни и създаването на директории за тях."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_set_path = './data/train'\n",
    "test_set_path = './data/test'\n",
    "valid_set_path = './data/valid'"
   ],
   "metadata": {
    "id": "V9VaiSUI6ARW"
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_set_path_5 = './train_5'\n",
    "test_set_path_5 = './test_5'\n",
    "valid_set_path_5 = './valid_5'"
   ],
   "metadata": {
    "id": "lXg7tuB2VPrY"
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def create_datasets(path, breeds_count):\n",
    "    data = load_files(path)\n",
    "    dog_files = np.array(data['filenames'])\n",
    "    dog_targets = np_utils.to_categorical(np.array(data['target']), breeds_count)\n",
    "    return dog_files, dog_targets"
   ],
   "metadata": {
    "id": "KtDdH2Ir-PlI"
   },
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "create_datasets функцията чете файловете на посочения път (path) и ги разпределя на входни данни и класове (labels). Чрез класовете ще проверим колко вярно класифицират моделите. За удобство чрез функцията np_utils.to_categorical() преобразуваме лейбълите във вектор с размер, равен на броя породи, който е съставен от нули и точно една единица, която ни казва каква е породата на съответното куче. Например, ако кучето е от първата категория, то на съответната позиция във вектора ще имаме 1-ца."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "X, Y = create_datasets(train_set_path_5, 5)\n",
    "X_valid, Y_valid = create_datasets(valid_set_path_5, 5)\n",
    "X_test, Y_test = create_datasets(test_set_path_5, 5)"
   ],
   "metadata": {
    "id": "t6Jzcpp3RGAK"
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def path_to_tensor(img_path):\n",
    "  img = image.load_img(img_path, target_size=(224, 224))\n",
    "  tensor = image.img_to_array(img)\n",
    "  return np.expand_dims(tensor, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "  list_of_tensors = [path_to_tensor(img_path) for img_path in img_paths]\n",
    "  return np.vstack(list_of_tensors)"
   ],
   "metadata": {
    "id": "4h9kQzRvPKIW"
   },
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Тъй като Keras очаква 4D масив (тензор) като вход, използваме двете функции path_to_tensor и paths_to_tensor, за да създадем точно такива 4D тензори, които да подадем на CNN. Тези тензори изглеждат така: (1, 224, 224, 3), като първото число съответства на броя снимки, второто и третото са брой редове и колони, а последното са броя цветови канали (тъй като работим със снимки в RGB формат, броят канали е 3)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "ImageFile.LOAD_TRUNCATED_IMAGES = True \n",
    "train_tensors = paths_to_tensor(X).astype('float32') / 255\n",
    "valid_tensors = paths_to_tensor(X_valid).astype('float32') / 255\n",
    "test_tensors = paths_to_tensor(X_test).astype('float32') / 255"
   ],
   "metadata": {
    "id": "OUTbbQwdfy1L"
   },
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Нуждаем се и от предварителна обработка на данните и затова скалираме снимките, като разделим всеки пиксел на всяка снимка на 255."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_tensors.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3g7s_2PJI119",
    "outputId": "a95ee28b-c343-456b-bf05-a9e851f65800"
   },
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "(314, 224, 224, 3)"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(224,224,3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(5, activation='relu'))\n",
    "model.summary()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oYPRPSHWJcoF",
    "outputId": "786b7c03-b175-4eb5-df83-6a868743d1ea"
   },
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 222, 222, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 111, 111, 16)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 109, 109, 32)      4640      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 54, 54, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 52, 52, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 26, 26, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 64)               0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,909\n",
      "Trainable params: 23,909\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Създаваме Sequential модел, който групира всички слоеве на мрежата. Мрежата е съставена от 8 слоя - 3 конволюционни, 3 max pooling, 1 average pooling и 1 dense.\n",
    "Конволюционните слоеве имат за цел да научат някакви черти (features) от снимките, например нос, очи, уста, като в случая това се случва с матрица (3, 3). Изхода от конволюционните слоеве се подава на max pooling слоевете, които имат за цел в матрица от пиксели с размерност (2, 2) да вземат този пиксел, чиято стойност е най-голяма и да добавят само тази стойност към изхода от слоя. Използваме max pooling слоевете, за да избегнем overfit-ване и да намалим размерността на изхода от конволюционните слоеве. Global average pooling слоя прави подобно нещо на max pooling само че взима средното. Dense слоя получава данни като вход от всички неврони на предишния слой и в случая служи като завършващ слой на мрежата, тоест след него получаваме резултат какво е класифицирала мрежата."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ],
   "metadata": {
    "id": "PUMN5L-kLQP-"
   },
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.my_model.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "model.fit(train_tensors, Y, \n",
    "          validation_data=(valid_tensors, Y_valid),\n",
    "          epochs=50, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l4RNacUALVIv",
    "outputId": "4f64a1c8-678b-432a-b811-9435b73fc106"
   },
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.9064 - accuracy: 0.2389\n",
      "Epoch 1: val_loss improved from inf to 6.87223, saving model to saved_models\\weights.best.my_model.hdf5\n",
      "16/16 [==============================] - 6s 298ms/step - loss: 6.9064 - accuracy: 0.2389 - val_loss: 6.8722 - val_accuracy: 0.2308\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.6030 - accuracy: 0.2261\n",
      "Epoch 2: val_loss did not improve from 6.87223\n",
      "16/16 [==============================] - 4s 272ms/step - loss: 6.6030 - accuracy: 0.2261 - val_loss: 6.8724 - val_accuracy: 0.2308\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.6000 - accuracy: 0.2293\n",
      "Epoch 3: val_loss improved from 6.87223 to 6.87209, saving model to saved_models\\weights.best.my_model.hdf5\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 6.6000 - accuracy: 0.2293 - val_loss: 6.8721 - val_accuracy: 0.2308\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.5983 - accuracy: 0.2420\n",
      "Epoch 4: val_loss did not improve from 6.87209\n",
      "16/16 [==============================] - 4s 265ms/step - loss: 6.5983 - accuracy: 0.2420 - val_loss: 6.8889 - val_accuracy: 0.2308\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.6038 - accuracy: 0.2357\n",
      "Epoch 5: val_loss improved from 6.87209 to 6.86087, saving model to saved_models\\weights.best.my_model.hdf5\n",
      "16/16 [==============================] - 4s 259ms/step - loss: 6.6038 - accuracy: 0.2357 - val_loss: 6.8609 - val_accuracy: 0.2308\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.5892 - accuracy: 0.2484\n",
      "Epoch 6: val_loss improved from 6.86087 to 6.85579, saving model to saved_models\\weights.best.my_model.hdf5\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 6.5892 - accuracy: 0.2484 - val_loss: 6.8558 - val_accuracy: 0.2821\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.5917 - accuracy: 0.2516\n",
      "Epoch 7: val_loss did not improve from 6.85579\n",
      "16/16 [==============================] - 4s 257ms/step - loss: 6.5917 - accuracy: 0.2516 - val_loss: 6.8738 - val_accuracy: 0.2308\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.5921 - accuracy: 0.2580\n",
      "Epoch 8: val_loss improved from 6.85579 to 6.84365, saving model to saved_models\\weights.best.my_model.hdf5\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 6.5921 - accuracy: 0.2580 - val_loss: 6.8437 - val_accuracy: 0.2308\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.5972 - accuracy: 0.2484\n",
      "Epoch 9: val_loss improved from 6.84365 to 6.83973, saving model to saved_models\\weights.best.my_model.hdf5\n",
      "16/16 [==============================] - 4s 277ms/step - loss: 6.5972 - accuracy: 0.2484 - val_loss: 6.8397 - val_accuracy: 0.3077\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.5710 - accuracy: 0.3057\n",
      "Epoch 10: val_loss did not improve from 6.83973\n",
      "16/16 [==============================] - 4s 274ms/step - loss: 6.5710 - accuracy: 0.3057 - val_loss: 6.8594 - val_accuracy: 0.2051\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.5684 - accuracy: 0.2771\n",
      "Epoch 11: val_loss improved from 6.83973 to 6.81158, saving model to saved_models\\weights.best.my_model.hdf5\n",
      "16/16 [==============================] - 5s 289ms/step - loss: 6.5684 - accuracy: 0.2771 - val_loss: 6.8116 - val_accuracy: 0.2821\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.5793 - accuracy: 0.2834\n",
      "Epoch 12: val_loss did not improve from 6.81158\n",
      "16/16 [==============================] - 4s 277ms/step - loss: 6.5793 - accuracy: 0.2834 - val_loss: 6.8194 - val_accuracy: 0.2564\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.5730 - accuracy: 0.2834\n",
      "Epoch 13: val_loss did not improve from 6.81158\n",
      "16/16 [==============================] - 5s 321ms/step - loss: 6.5730 - accuracy: 0.2834 - val_loss: 6.8326 - val_accuracy: 0.2821\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.5576 - accuracy: 0.2834\n",
      "Epoch 14: val_loss improved from 6.81158 to 6.81060, saving model to saved_models\\weights.best.my_model.hdf5\n",
      "16/16 [==============================] - 5s 334ms/step - loss: 6.5576 - accuracy: 0.2834 - val_loss: 6.8106 - val_accuracy: 0.2821\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.5493 - accuracy: 0.2994\n",
      "Epoch 15: val_loss did not improve from 6.81060\n",
      "16/16 [==============================] - 4s 272ms/step - loss: 6.5493 - accuracy: 0.2994 - val_loss: 7.2119 - val_accuracy: 0.3077\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.6378 - accuracy: 0.3025\n",
      "Epoch 16: val_loss did not improve from 6.81060\n",
      "16/16 [==============================] - 4s 278ms/step - loss: 6.6378 - accuracy: 0.3025 - val_loss: 6.8303 - val_accuracy: 0.2308\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.5511 - accuracy: 0.2898\n",
      "Epoch 17: val_loss did not improve from 6.81060\n",
      "16/16 [==============================] - 4s 278ms/step - loss: 6.5511 - accuracy: 0.2898 - val_loss: 6.8265 - val_accuracy: 0.2308\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.5469 - accuracy: 0.2962\n",
      "Epoch 18: val_loss did not improve from 6.81060\n",
      "16/16 [==============================] - 4s 272ms/step - loss: 6.5469 - accuracy: 0.2962 - val_loss: 6.8153 - val_accuracy: 0.2821\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.5352 - accuracy: 0.3089\n",
      "Epoch 19: val_loss improved from 6.81060 to 6.79219, saving model to saved_models\\weights.best.my_model.hdf5\n",
      "16/16 [==============================] - 4s 277ms/step - loss: 6.5352 - accuracy: 0.3089 - val_loss: 6.7922 - val_accuracy: 0.3333\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.5343 - accuracy: 0.2930\n",
      "Epoch 20: val_loss improved from 6.79219 to 6.78118, saving model to saved_models\\weights.best.my_model.hdf5\n",
      "16/16 [==============================] - 5s 283ms/step - loss: 6.5343 - accuracy: 0.2930 - val_loss: 6.7812 - val_accuracy: 0.3333\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.5480 - accuracy: 0.3057\n",
      "Epoch 21: val_loss improved from 6.78118 to 6.78095, saving model to saved_models\\weights.best.my_model.hdf5\n",
      "16/16 [==============================] - 4s 273ms/step - loss: 6.5480 - accuracy: 0.3057 - val_loss: 6.7810 - val_accuracy: 0.3333\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.5308 - accuracy: 0.2994\n",
      "Epoch 22: val_loss improved from 6.78095 to 6.77650, saving model to saved_models\\weights.best.my_model.hdf5\n",
      "16/16 [==============================] - 4s 274ms/step - loss: 6.5308 - accuracy: 0.2994 - val_loss: 6.7765 - val_accuracy: 0.3077\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.5477 - accuracy: 0.3057\n",
      "Epoch 23: val_loss improved from 6.77650 to 6.75840, saving model to saved_models\\weights.best.my_model.hdf5\n",
      "16/16 [==============================] - 4s 274ms/step - loss: 6.5477 - accuracy: 0.3057 - val_loss: 6.7584 - val_accuracy: 0.3333\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.6221 - accuracy: 0.3153\n",
      "Epoch 24: val_loss did not improve from 6.75840\n",
      "16/16 [==============================] - 4s 273ms/step - loss: 6.6221 - accuracy: 0.3153 - val_loss: 7.0954 - val_accuracy: 0.3846\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.5914 - accuracy: 0.2803\n",
      "Epoch 25: val_loss did not improve from 6.75840\n",
      "16/16 [==============================] - 4s 277ms/step - loss: 6.5914 - accuracy: 0.2803 - val_loss: 6.7701 - val_accuracy: 0.3077\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.5507 - accuracy: 0.3312\n",
      "Epoch 26: val_loss did not improve from 6.75840\n",
      "16/16 [==============================] - 6s 373ms/step - loss: 6.5507 - accuracy: 0.3312 - val_loss: 6.7694 - val_accuracy: 0.3333\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.4941 - accuracy: 0.3599\n",
      "Epoch 27: val_loss did not improve from 6.75840\n",
      "16/16 [==============================] - 5s 322ms/step - loss: 6.4941 - accuracy: 0.3599 - val_loss: 7.1081 - val_accuracy: 0.3333\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.6138 - accuracy: 0.3567\n",
      "Epoch 28: val_loss improved from 6.75840 to 6.75062, saving model to saved_models\\weights.best.my_model.hdf5\n",
      "16/16 [==============================] - 5s 283ms/step - loss: 6.6138 - accuracy: 0.3567 - val_loss: 6.7506 - val_accuracy: 0.3077\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.5931 - accuracy: 0.2484\n",
      "Epoch 29: val_loss did not improve from 6.75062\n",
      "16/16 [==============================] - 4s 264ms/step - loss: 6.5931 - accuracy: 0.2484 - val_loss: 6.8279 - val_accuracy: 0.2564\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.5295 - accuracy: 0.3280\n",
      "Epoch 30: val_loss did not improve from 6.75062\n",
      "16/16 [==============================] - 4s 268ms/step - loss: 6.5295 - accuracy: 0.3280 - val_loss: 6.7856 - val_accuracy: 0.3333\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.4776 - accuracy: 0.3344\n",
      "Epoch 31: val_loss did not improve from 6.75062\n",
      "16/16 [==============================] - 4s 269ms/step - loss: 6.4776 - accuracy: 0.3344 - val_loss: 7.6877 - val_accuracy: 0.2821\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.5216 - accuracy: 0.3471\n",
      "Epoch 32: val_loss did not improve from 6.75062\n",
      "16/16 [==============================] - 4s 268ms/step - loss: 6.5216 - accuracy: 0.3471 - val_loss: 6.7912 - val_accuracy: 0.3333\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.5324 - accuracy: 0.3599\n",
      "Epoch 33: val_loss improved from 6.75062 to 6.73966, saving model to saved_models\\weights.best.my_model.hdf5\n",
      "16/16 [==============================] - 4s 265ms/step - loss: 6.5324 - accuracy: 0.3599 - val_loss: 6.7397 - val_accuracy: 0.3333\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.5079 - accuracy: 0.3344\n",
      "Epoch 34: val_loss improved from 6.73966 to 6.70159, saving model to saved_models\\weights.best.my_model.hdf5\n",
      "16/16 [==============================] - 4s 268ms/step - loss: 6.5079 - accuracy: 0.3344 - val_loss: 6.7016 - val_accuracy: 0.3333\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.6034 - accuracy: 0.3535\n",
      "Epoch 35: val_loss did not improve from 6.70159\n",
      "16/16 [==============================] - 4s 274ms/step - loss: 6.6034 - accuracy: 0.3535 - val_loss: 7.4572 - val_accuracy: 0.3333\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.5650 - accuracy: 0.3503\n",
      "Epoch 36: val_loss did not improve from 6.70159\n",
      "16/16 [==============================] - 4s 269ms/step - loss: 6.5650 - accuracy: 0.3503 - val_loss: 6.7442 - val_accuracy: 0.3590\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.5195 - accuracy: 0.3949\n",
      "Epoch 37: val_loss did not improve from 6.70159\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 6.5195 - accuracy: 0.3949 - val_loss: 6.7181 - val_accuracy: 0.3846\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.5463 - accuracy: 0.3599\n",
      "Epoch 38: val_loss did not improve from 6.70159\n",
      "16/16 [==============================] - 4s 266ms/step - loss: 6.5463 - accuracy: 0.3599 - val_loss: 6.7581 - val_accuracy: 0.3590\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.4985 - accuracy: 0.3694\n",
      "Epoch 39: val_loss did not improve from 6.70159\n",
      "16/16 [==============================] - 4s 263ms/step - loss: 6.4985 - accuracy: 0.3694 - val_loss: 6.7100 - val_accuracy: 0.3333\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.4983 - accuracy: 0.3439\n",
      "Epoch 40: val_loss did not improve from 6.70159\n",
      "16/16 [==============================] - 4s 266ms/step - loss: 6.4983 - accuracy: 0.3439 - val_loss: 6.7432 - val_accuracy: 0.3846\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.4561 - accuracy: 0.3599\n",
      "Epoch 41: val_loss did not improve from 6.70159\n",
      "16/16 [==============================] - 4s 268ms/step - loss: 6.4561 - accuracy: 0.3599 - val_loss: 6.7078 - val_accuracy: 0.3333\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.4851 - accuracy: 0.3471\n",
      "Epoch 42: val_loss did not improve from 6.70159\n",
      "16/16 [==============================] - 4s 270ms/step - loss: 6.4851 - accuracy: 0.3471 - val_loss: 6.7732 - val_accuracy: 0.3333\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.6450 - accuracy: 0.3726\n",
      "Epoch 43: val_loss did not improve from 6.70159\n",
      "16/16 [==============================] - 4s 266ms/step - loss: 6.6450 - accuracy: 0.3726 - val_loss: 7.4436 - val_accuracy: 0.3333\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.5050 - accuracy: 0.3662\n",
      "Epoch 44: val_loss improved from 6.70159 to 6.65783, saving model to saved_models\\weights.best.my_model.hdf5\n",
      "16/16 [==============================] - 4s 273ms/step - loss: 6.5050 - accuracy: 0.3662 - val_loss: 6.6578 - val_accuracy: 0.4103\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.4655 - accuracy: 0.3885\n",
      "Epoch 45: val_loss did not improve from 6.65783\n",
      "16/16 [==============================] - 4s 270ms/step - loss: 6.4655 - accuracy: 0.3885 - val_loss: 6.6863 - val_accuracy: 0.4615\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.4776 - accuracy: 0.3758\n",
      "Epoch 46: val_loss did not improve from 6.65783\n",
      "16/16 [==============================] - 4s 267ms/step - loss: 6.4776 - accuracy: 0.3758 - val_loss: 6.7028 - val_accuracy: 0.4359\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.5804 - accuracy: 0.4076\n",
      "Epoch 47: val_loss improved from 6.65783 to 6.63001, saving model to saved_models\\weights.best.my_model.hdf5\n",
      "16/16 [==============================] - 4s 266ms/step - loss: 6.5804 - accuracy: 0.4076 - val_loss: 6.6300 - val_accuracy: 0.4359\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - ETA: 0s - loss: 7.4682 - accuracy: 0.3312\n",
      "Epoch 48: val_loss did not improve from 6.63001\n",
      "16/16 [==============================] - 4s 266ms/step - loss: 7.4682 - accuracy: 0.3312 - val_loss: 7.9546 - val_accuracy: 0.2564\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.9690 - accuracy: 0.3057\n",
      "Epoch 49: val_loss did not improve from 6.63001\n",
      "16/16 [==============================] - 4s 270ms/step - loss: 6.9690 - accuracy: 0.3057 - val_loss: 6.8129 - val_accuracy: 0.3077\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.4986 - accuracy: 0.3408\n",
      "Epoch 50: val_loss did not improve from 6.63001\n",
      "16/16 [==============================] - 4s 265ms/step - loss: 6.4986 - accuracy: 0.3408 - val_loss: 6.7441 - val_accuracy: 0.3333\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x1740304b2e0>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Чрез горния код тренираме съставения модел с тренировъчните данни и валидираме с валидиращите данни, като изпълняваме 50 епохи и най-добрия резултат бива записан във файл."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model.load_weights('saved_models/weights.best.my_model.hdf5')"
   ],
   "metadata": {
    "id": "xrkfIl_2e7gc"
   },
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "print(f\"Accuracy: {accuracy_score(Y_test, np_utils.to_categorical(dog_breed_predictions, 5))*100}%\")\n",
    "print(f\"Recall: {recall_score(Y_test, np_utils.to_categorical(dog_breed_predictions, 5), average='weighted', zero_division=1)*100}%\")\n",
    "print(f\"Precision: {precision_score(Y_test, np_utils.to_categorical(dog_breed_predictions, 5), average='weighted', zero_division=1)*100}%\")\n",
    "print(f\"F1 score: {f1_score(Y_test, np_utils.to_categorical(dog_breed_predictions, 5), average='weighted', zero_division=1)*100}%\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tz9iRSoifBgZ",
    "outputId": "5095ad69-9e30-419f-fb98-45e873c4d53c"
   },
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Accuracy: 37.5%\n",
      "Recall: 37.5%\n",
      "Precision: 64.14596273291926%\n",
      "F1 score: 30.908602150537632%\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Накрая принтираме получените резултати от тестовите данни и можем да забележим, че резултатът не е толкова добър, въпреки че породите, които използваме са само 5 на брой. Според мен, ниският резултат се дължи на това, че всяка една от тези 5 породи разползага със средно около 50 тренировъчни снимки и мрежата не успява да научи достатъчно feature-и и да класифицира по-добре кучетата. Правим абсолютно същите стъпки и за тренирането на CNN с 10 породи:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('data/train/train10.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('train_10')\n",
    "with zipfile.ZipFile('data/test/test10.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('test_10')\n",
    "with zipfile.ZipFile('data/valid/valid10.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('valid_10')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_set_path_10 = './train_10'\n",
    "test_set_path_10 = './test_10'\n",
    "valid_set_path_10 = './valid_10'"
   ],
   "metadata": {
    "id": "8L8WObT-hUuv"
   },
   "execution_count": 45,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "X, Y = create_datasets(train_set_path_10, 10)\n",
    "X_valid, Y_valid = create_datasets(valid_set_path_10, 10)\n",
    "X_test, Y_test = create_datasets(test_set_path_10, 10)"
   ],
   "metadata": {
    "id": "zrPzQyGNiNE9"
   },
   "execution_count": 48,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "train_tensors = paths_to_tensor(X).astype('float32') / 255\n",
    "valid_tensors = paths_to_tensor(X_valid).astype('float32') / 255\n",
    "test_tensors = paths_to_tensor(X_test).astype('float32') / 255"
   ],
   "metadata": {
    "id": "1fEcCKlOiZ4v"
   },
   "execution_count": 49,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(224,224,3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.summary()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0AgpCCvsjQHh",
    "outputId": "d4db7da3-9c30-42e4-eca0-ded0a949d8d8"
   },
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 222, 222, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 111, 111, 16)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 109, 109, 32)      4640      \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 54, 54, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 52, 52, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 26, 26, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " global_average_pooling2d_1   (None, 64)               0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,234\n",
      "Trainable params: 24,234\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ],
   "metadata": {
    "id": "qC4ladngjZTD"
   },
   "execution_count": 51,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.my_model_10.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(train_tensors, Y, \n",
    "          validation_data=(valid_tensors, Y_valid),\n",
    "          epochs=50, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HGwvX6ZZjcVA",
    "outputId": "812607fa-2e3b-47fc-979a-12ba6f200045"
   },
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 8.1194 - accuracy: 0.0986\n",
      "Epoch 1: val_loss improved from inf to 8.26613, saving model to saved_models\\weights.best.my_model_10.hdf5\n",
      "29/29 [==============================] - 11s 311ms/step - loss: 8.1194 - accuracy: 0.0986 - val_loss: 8.2661 - val_accuracy: 0.1127\n",
      "Epoch 2/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 7.6337 - accuracy: 0.0917\n",
      "Epoch 2: val_loss improved from 8.26613 to 6.93199, saving model to saved_models\\weights.best.my_model_10.hdf5\n",
      "29/29 [==============================] - 8s 259ms/step - loss: 7.6337 - accuracy: 0.0917 - val_loss: 6.9320 - val_accuracy: 0.0845\n",
      "Epoch 3/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 6.9727 - accuracy: 0.1038\n",
      "Epoch 3: val_loss did not improve from 6.93199\n",
      "29/29 [==============================] - 7s 256ms/step - loss: 6.9727 - accuracy: 0.1038 - val_loss: 6.9385 - val_accuracy: 0.1127\n",
      "Epoch 4/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 6.9709 - accuracy: 0.1055\n",
      "Epoch 4: val_loss improved from 6.93199 to 6.92908, saving model to saved_models\\weights.best.my_model_10.hdf5\n",
      "29/29 [==============================] - 9s 306ms/step - loss: 6.9709 - accuracy: 0.1055 - val_loss: 6.9291 - val_accuracy: 0.1127\n",
      "Epoch 5/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 6.9726 - accuracy: 0.0917\n",
      "Epoch 5: val_loss improved from 6.92908 to 6.92370, saving model to saved_models\\weights.best.my_model_10.hdf5\n",
      "29/29 [==============================] - 8s 282ms/step - loss: 6.9726 - accuracy: 0.0917 - val_loss: 6.9237 - val_accuracy: 0.1268\n",
      "Epoch 6/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 6.9613 - accuracy: 0.1315\n",
      "Epoch 6: val_loss improved from 6.92370 to 6.92131, saving model to saved_models\\weights.best.my_model_10.hdf5\n",
      "29/29 [==============================] - 8s 280ms/step - loss: 6.9613 - accuracy: 0.1315 - val_loss: 6.9213 - val_accuracy: 0.0986\n",
      "Epoch 7/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 6.9474 - accuracy: 0.1280\n",
      "Epoch 7: val_loss improved from 6.92131 to 6.90347, saving model to saved_models\\weights.best.my_model_10.hdf5\n",
      "29/29 [==============================] - 8s 273ms/step - loss: 6.9474 - accuracy: 0.1280 - val_loss: 6.9035 - val_accuracy: 0.1268\n",
      "Epoch 8/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 6.9473 - accuracy: 0.1211\n",
      "Epoch 8: val_loss did not improve from 6.90347\n",
      "29/29 [==============================] - 8s 273ms/step - loss: 6.9473 - accuracy: 0.1211 - val_loss: 6.9110 - val_accuracy: 0.1268\n",
      "Epoch 9/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 6.9286 - accuracy: 0.1142\n",
      "Epoch 9: val_loss did not improve from 6.90347\n",
      "29/29 [==============================] - 8s 271ms/step - loss: 6.9286 - accuracy: 0.1142 - val_loss: 6.9217 - val_accuracy: 0.1127\n",
      "Epoch 10/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 6.9454 - accuracy: 0.1367\n",
      "Epoch 10: val_loss improved from 6.90347 to 6.89930, saving model to saved_models\\weights.best.my_model_10.hdf5\n",
      "29/29 [==============================] - 8s 274ms/step - loss: 6.9454 - accuracy: 0.1367 - val_loss: 6.8993 - val_accuracy: 0.1408\n",
      "Epoch 11/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 6.9561 - accuracy: 0.1557\n",
      "Epoch 11: val_loss improved from 6.89930 to 6.89476, saving model to saved_models\\weights.best.my_model_10.hdf5\n",
      "29/29 [==============================] - 8s 273ms/step - loss: 6.9561 - accuracy: 0.1557 - val_loss: 6.8948 - val_accuracy: 0.1268\n",
      "Epoch 12/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 6.9940 - accuracy: 0.1505\n",
      "Epoch 12: val_loss did not improve from 6.89476\n",
      "29/29 [==============================] - 8s 280ms/step - loss: 6.9940 - accuracy: 0.1505 - val_loss: 7.0909 - val_accuracy: 0.1690\n",
      "Epoch 13/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 6.7353 - accuracy: 0.1522\n",
      "Epoch 13: val_loss improved from 6.89476 to 5.18279, saving model to saved_models\\weights.best.my_model_10.hdf5\n",
      "29/29 [==============================] - 8s 274ms/step - loss: 6.7353 - accuracy: 0.1522 - val_loss: 5.1828 - val_accuracy: 0.1408\n",
      "Epoch 14/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 4.0183 - accuracy: 0.1280\n",
      "Epoch 14: val_loss improved from 5.18279 to 3.90543, saving model to saved_models\\weights.best.my_model_10.hdf5\n",
      "29/29 [==============================] - 8s 293ms/step - loss: 4.0183 - accuracy: 0.1280 - val_loss: 3.9054 - val_accuracy: 0.1268\n",
      "Epoch 15/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 3.6300 - accuracy: 0.1920\n",
      "Epoch 15: val_loss improved from 3.90543 to 3.85697, saving model to saved_models\\weights.best.my_model_10.hdf5\n",
      "29/29 [==============================] - 8s 274ms/step - loss: 3.6300 - accuracy: 0.1920 - val_loss: 3.8570 - val_accuracy: 0.1549\n",
      "Epoch 16/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 3.6252 - accuracy: 0.1903\n",
      "Epoch 16: val_loss improved from 3.85697 to 3.85259, saving model to saved_models\\weights.best.my_model_10.hdf5\n",
      "29/29 [==============================] - 8s 271ms/step - loss: 3.6252 - accuracy: 0.1903 - val_loss: 3.8526 - val_accuracy: 0.1408\n",
      "Epoch 17/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 3.2359 - accuracy: 0.1644\n",
      "Epoch 17: val_loss improved from 3.85259 to 2.40104, saving model to saved_models\\weights.best.my_model_10.hdf5\n",
      "29/29 [==============================] - 8s 274ms/step - loss: 3.2359 - accuracy: 0.1644 - val_loss: 2.4010 - val_accuracy: 0.1690\n",
      "Epoch 18/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 2.1440 - accuracy: 0.2007\n",
      "Epoch 18: val_loss improved from 2.40104 to 2.32936, saving model to saved_models\\weights.best.my_model_10.hdf5\n",
      "29/29 [==============================] - 8s 272ms/step - loss: 2.1440 - accuracy: 0.2007 - val_loss: 2.3294 - val_accuracy: 0.2254\n",
      "Epoch 19/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 2.2508 - accuracy: 0.1972\n",
      "Epoch 19: val_loss improved from 2.32936 to 2.28157, saving model to saved_models\\weights.best.my_model_10.hdf5\n",
      "29/29 [==============================] - 8s 272ms/step - loss: 2.2508 - accuracy: 0.1972 - val_loss: 2.2816 - val_accuracy: 0.1690\n",
      "Epoch 20/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 2.1577 - accuracy: 0.1799\n",
      "Epoch 20: val_loss did not improve from 2.28157\n",
      "29/29 [==============================] - 8s 278ms/step - loss: 2.1577 - accuracy: 0.1799 - val_loss: 2.3882 - val_accuracy: 0.1690\n",
      "Epoch 21/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 2.1610 - accuracy: 0.2180\n",
      "Epoch 21: val_loss did not improve from 2.28157\n",
      "29/29 [==============================] - 8s 278ms/step - loss: 2.1610 - accuracy: 0.2180 - val_loss: 3.1585 - val_accuracy: 0.1831\n",
      "Epoch 22/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 2.2071 - accuracy: 0.2128\n",
      "Epoch 22: val_loss did not improve from 2.28157\n",
      "29/29 [==============================] - 8s 283ms/step - loss: 2.2071 - accuracy: 0.2128 - val_loss: 2.8881 - val_accuracy: 0.1408\n",
      "Epoch 23/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 2.2684 - accuracy: 0.1990\n",
      "Epoch 23: val_loss did not improve from 2.28157\n",
      "29/29 [==============================] - 8s 270ms/step - loss: 2.2684 - accuracy: 0.1990 - val_loss: 2.3526 - val_accuracy: 0.1972\n",
      "Epoch 24/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 2.1899 - accuracy: 0.1886\n",
      "Epoch 24: val_loss did not improve from 2.28157\n",
      "29/29 [==============================] - 8s 270ms/step - loss: 2.1899 - accuracy: 0.1886 - val_loss: 2.3342 - val_accuracy: 0.2113\n",
      "Epoch 25/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 2.1594 - accuracy: 0.2526\n",
      "Epoch 25: val_loss did not improve from 2.28157\n",
      "29/29 [==============================] - 8s 273ms/step - loss: 2.1594 - accuracy: 0.2526 - val_loss: 2.3047 - val_accuracy: 0.1831\n",
      "Epoch 26/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 2.1372 - accuracy: 0.2093\n",
      "Epoch 26: val_loss did not improve from 2.28157\n",
      "29/29 [==============================] - 8s 270ms/step - loss: 2.1372 - accuracy: 0.2093 - val_loss: 2.5052 - val_accuracy: 0.1408\n",
      "Epoch 27/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 2.1325 - accuracy: 0.2093\n",
      "Epoch 27: val_loss did not improve from 2.28157\n",
      "29/29 [==============================] - 8s 270ms/step - loss: 2.1325 - accuracy: 0.2093 - val_loss: 2.4351 - val_accuracy: 0.2394\n",
      "Epoch 28/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 2.1515 - accuracy: 0.2284\n",
      "Epoch 28: val_loss did not improve from 2.28157\n",
      "29/29 [==============================] - 8s 276ms/step - loss: 2.1515 - accuracy: 0.2284 - val_loss: 2.4351 - val_accuracy: 0.1972\n",
      "Epoch 29/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 2.2442 - accuracy: 0.2370\n",
      "Epoch 29: val_loss did not improve from 2.28157\n",
      "29/29 [==============================] - 8s 276ms/step - loss: 2.2442 - accuracy: 0.2370 - val_loss: 2.4758 - val_accuracy: 0.1549\n",
      "Epoch 30/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 2.2264 - accuracy: 0.2474\n",
      "Epoch 30: val_loss did not improve from 2.28157\n",
      "29/29 [==============================] - 8s 272ms/step - loss: 2.2264 - accuracy: 0.2474 - val_loss: 2.2910 - val_accuracy: 0.2535\n",
      "Epoch 31/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 2.1321 - accuracy: 0.2353\n",
      "Epoch 31: val_loss did not improve from 2.28157\n",
      "29/29 [==============================] - 8s 272ms/step - loss: 2.1321 - accuracy: 0.2353 - val_loss: 2.4882 - val_accuracy: 0.2254\n",
      "Epoch 32/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 2.1014 - accuracy: 0.2526\n",
      "Epoch 32: val_loss did not improve from 2.28157\n",
      "29/29 [==============================] - 9s 315ms/step - loss: 2.1014 - accuracy: 0.2526 - val_loss: 2.4743 - val_accuracy: 0.2254\n",
      "Epoch 33/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 2.3409 - accuracy: 0.2405\n",
      "Epoch 33: val_loss did not improve from 2.28157\n",
      "29/29 [==============================] - 9s 302ms/step - loss: 2.3409 - accuracy: 0.2405 - val_loss: 2.4929 - val_accuracy: 0.2113\n",
      "Epoch 34/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 2.0737 - accuracy: 0.2561\n",
      "Epoch 34: val_loss did not improve from 2.28157\n",
      "29/29 [==============================] - 8s 279ms/step - loss: 2.0737 - accuracy: 0.2561 - val_loss: 2.9569 - val_accuracy: 0.1972\n",
      "Epoch 35/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 2.1636 - accuracy: 0.2266\n",
      "Epoch 35: val_loss did not improve from 2.28157\n",
      "29/29 [==============================] - 8s 291ms/step - loss: 2.1636 - accuracy: 0.2266 - val_loss: 2.4340 - val_accuracy: 0.1831\n",
      "Epoch 36/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 2.0989 - accuracy: 0.2578\n",
      "Epoch 36: val_loss improved from 2.28157 to 2.26685, saving model to saved_models\\weights.best.my_model_10.hdf5\n",
      "29/29 [==============================] - 8s 290ms/step - loss: 2.0989 - accuracy: 0.2578 - val_loss: 2.2668 - val_accuracy: 0.2394\n",
      "Epoch 37/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 2.0974 - accuracy: 0.2682\n",
      "Epoch 37: val_loss did not improve from 2.26685\n",
      "29/29 [==============================] - 9s 296ms/step - loss: 2.0974 - accuracy: 0.2682 - val_loss: 2.8019 - val_accuracy: 0.2394\n",
      "Epoch 38/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 2.0610 - accuracy: 0.2301\n",
      "Epoch 38: val_loss improved from 2.26685 to 2.25469, saving model to saved_models\\weights.best.my_model_10.hdf5\n",
      "29/29 [==============================] - 9s 301ms/step - loss: 2.0610 - accuracy: 0.2301 - val_loss: 2.2547 - val_accuracy: 0.1831\n",
      "Epoch 39/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 2.1916 - accuracy: 0.2284\n",
      "Epoch 39: val_loss improved from 2.25469 to 2.24696, saving model to saved_models\\weights.best.my_model_10.hdf5\n",
      "29/29 [==============================] - 9s 303ms/step - loss: 2.1916 - accuracy: 0.2284 - val_loss: 2.2470 - val_accuracy: 0.1972\n",
      "Epoch 40/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 2.1676 - accuracy: 0.2612\n",
      "Epoch 40: val_loss did not improve from 2.24696\n",
      "29/29 [==============================] - 9s 305ms/step - loss: 2.1676 - accuracy: 0.2612 - val_loss: 2.3863 - val_accuracy: 0.2676\n",
      "Epoch 41/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 2.1322 - accuracy: 0.2543\n",
      "Epoch 41: val_loss did not improve from 2.24696\n",
      "29/29 [==============================] - 8s 286ms/step - loss: 2.1322 - accuracy: 0.2543 - val_loss: 2.7510 - val_accuracy: 0.2535\n",
      "Epoch 42/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 2.3405 - accuracy: 0.2785\n",
      "Epoch 42: val_loss did not improve from 2.24696\n",
      "29/29 [==============================] - 8s 266ms/step - loss: 2.3405 - accuracy: 0.2785 - val_loss: 2.9631 - val_accuracy: 0.1831\n",
      "Epoch 43/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 2.0408 - accuracy: 0.2145\n",
      "Epoch 43: val_loss did not improve from 2.24696\n",
      "29/29 [==============================] - 8s 275ms/step - loss: 2.0408 - accuracy: 0.2145 - val_loss: 2.2586 - val_accuracy: 0.2394\n",
      "Epoch 44/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 2.1523 - accuracy: 0.2699\n",
      "Epoch 44: val_loss did not improve from 2.24696\n",
      "29/29 [==============================] - 8s 268ms/step - loss: 2.1523 - accuracy: 0.2699 - val_loss: 3.1580 - val_accuracy: 0.2254\n",
      "Epoch 45/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 2.0700 - accuracy: 0.2318\n",
      "Epoch 45: val_loss did not improve from 2.24696\n",
      "29/29 [==============================] - 8s 267ms/step - loss: 2.0700 - accuracy: 0.2318 - val_loss: 2.3768 - val_accuracy: 0.2254\n",
      "Epoch 46/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 2.6485 - accuracy: 0.2612\n",
      "Epoch 46: val_loss did not improve from 2.24696\n",
      "29/29 [==============================] - 8s 269ms/step - loss: 2.6485 - accuracy: 0.2612 - val_loss: 2.5359 - val_accuracy: 0.3099\n",
      "Epoch 47/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 2.3455 - accuracy: 0.1851\n",
      "Epoch 47: val_loss did not improve from 2.24696\n",
      "29/29 [==============================] - 8s 270ms/step - loss: 2.3455 - accuracy: 0.1851 - val_loss: 2.4131 - val_accuracy: 0.2394\n",
      "Epoch 48/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 2.0123 - accuracy: 0.2612\n",
      "Epoch 48: val_loss did not improve from 2.24696\n",
      "29/29 [==============================] - 8s 267ms/step - loss: 2.0123 - accuracy: 0.2612 - val_loss: 2.7238 - val_accuracy: 0.3099\n",
      "Epoch 49/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 2.3104 - accuracy: 0.2266\n",
      "Epoch 49: val_loss did not improve from 2.24696\n",
      "29/29 [==============================] - 8s 265ms/step - loss: 2.3104 - accuracy: 0.2266 - val_loss: 2.5055 - val_accuracy: 0.1268\n",
      "Epoch 50/50\n",
      "29/29 [==============================] - ETA: 0s - loss: 2.2200 - accuracy: 0.1505\n",
      "Epoch 50: val_loss did not improve from 2.24696\n",
      "29/29 [==============================] - 8s 265ms/step - loss: 2.2200 - accuracy: 0.1505 - val_loss: 2.4043 - val_accuracy: 0.1972\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x17403922f10>"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model.load_weights('saved_models/weights.best.my_model_10.hdf5')"
   ],
   "metadata": {
    "id": "ML19AW4cjrAm"
   },
   "execution_count": 53,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "print(f\"Accuracy: {accuracy_score(Y_test, np_utils.to_categorical(dog_breed_predictions, 10))*100}%\")\n",
    "print(f\"Recall: {recall_score(Y_test, np_utils.to_categorical(dog_breed_predictions, 10), average='weighted', zero_division=1)*100}%\")\n",
    "print(f\"Precision: {precision_score(Y_test, np_utils.to_categorical(dog_breed_predictions, 10), average='weighted', zero_division=1)*100}%\")\n",
    "print(f\"F1 score: {f1_score(Y_test, np_utils.to_categorical(dog_breed_predictions, 10), average='weighted', zero_division=1)*100}%\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qHtlWcF6jvAY",
    "outputId": "73ef593e-7a55-4972-dc09-4470b1f10eae"
   },
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Accuracy: 19.17808219178082%\n",
      "Recall: 19.17808219178082%\n",
      "Precision: 36.66824122185483%\n",
      "F1 score: 12.215320910973086%\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "За съжаление, резултатите са още по-ниски от тези на мрежата за 5 породи, което отново показва, че наличието на повече породи затруднява модела и влошава метриките.\n",
    "Има начин да получим и добър резултат с данните, които имаме налични и той е като използваме метода Transfer learning. Нарича се Transfer learning, защото използваме предварително добре натрениран модел върху голям брой снимки. Избрала съм модела ResNet50. Добавяме и два слоя към претренирания модел."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "X, Y = create_datasets(train_set_path, 133)\n",
    "X_valid, Y_valid = create_datasets(valid_set_path, 133)\n",
    "X_test, Y_test = create_datasets(test_set_path, 133)"
   ],
   "metadata": {
    "id": "a0O3HI0EsO9n"
   },
   "execution_count": 57,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "Resnet50_bottleneck_features = np.load('./DogResnet50Data.npz')\n",
    "train_Resnet50 = Resnet50_bottleneck_features['train']\n",
    "valid_Resnet50 = Resnet50_bottleneck_features['valid']\n",
    "test_Resnet50 = Resnet50_bottleneck_features['test']"
   ],
   "metadata": {
    "id": "p9piQNyTZIr5"
   },
   "execution_count": 59,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "Resnet50_model = Sequential()\n",
    "Resnet50_model.add(GlobalAveragePooling2D(input_shape=train_Resnet50.shape[1:]))\n",
    "Resnet50_model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "Resnet50_model.summary()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LCIVsC-iSLh2",
    "outputId": "8ab4966a-8bcb-499c-a064-cbacc758f905"
   },
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " global_average_pooling2d_2   (None, 2048)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 133)               272517    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 272,517\n",
      "Trainable params: 272,517\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "Resnet50_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ],
   "metadata": {
    "id": "aG9DeMXFW8FF"
   },
   "execution_count": 61,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.Resnet50.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "Resnet50_model.fit(train_Resnet50, Y, \n",
    "          validation_data=(valid_Resnet50, Y_valid),\n",
    "          epochs=20, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TCGbazRuXIwE",
    "outputId": "62270757-29f3-4af8-8c7b-d21ee3c761ae"
   },
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "327/334 [============================>.] - ETA: 0s - loss: 1.6296 - accuracy: 0.5963\n",
      "Epoch 1: val_loss improved from inf to 0.77463, saving model to saved_models\\weights.best.Resnet50.hdf5\n",
      "334/334 [==============================] - 3s 5ms/step - loss: 1.6161 - accuracy: 0.5987 - val_loss: 0.7746 - val_accuracy: 0.7689\n",
      "Epoch 2/20\n",
      "317/334 [===========================>..] - ETA: 0s - loss: 0.4398 - accuracy: 0.8637\n",
      "Epoch 2: val_loss improved from 0.77463 to 0.73950, saving model to saved_models\\weights.best.Resnet50.hdf5\n",
      "334/334 [==============================] - 1s 4ms/step - loss: 0.4349 - accuracy: 0.8642 - val_loss: 0.7395 - val_accuracy: 0.7725\n",
      "Epoch 3/20\n",
      "332/334 [============================>.] - ETA: 0s - loss: 0.2592 - accuracy: 0.9169\n",
      "Epoch 3: val_loss improved from 0.73950 to 0.70842, saving model to saved_models\\weights.best.Resnet50.hdf5\n",
      "334/334 [==============================] - 1s 4ms/step - loss: 0.2606 - accuracy: 0.9162 - val_loss: 0.7084 - val_accuracy: 0.7725\n",
      "Epoch 4/20\n",
      "333/334 [============================>.] - ETA: 0s - loss: 0.1710 - accuracy: 0.9470\n",
      "Epoch 4: val_loss improved from 0.70842 to 0.64733, saving model to saved_models\\weights.best.Resnet50.hdf5\n",
      "334/334 [==============================] - 1s 4ms/step - loss: 0.1706 - accuracy: 0.9472 - val_loss: 0.6473 - val_accuracy: 0.8120\n",
      "Epoch 5/20\n",
      "322/334 [===========================>..] - ETA: 0s - loss: 0.1132 - accuracy: 0.9677\n",
      "Epoch 5: val_loss improved from 0.64733 to 0.62386, saving model to saved_models\\weights.best.Resnet50.hdf5\n",
      "334/334 [==============================] - 1s 4ms/step - loss: 0.1146 - accuracy: 0.9665 - val_loss: 0.6239 - val_accuracy: 0.8108\n",
      "Epoch 6/20\n",
      "333/334 [============================>.] - ETA: 0s - loss: 0.0798 - accuracy: 0.9779\n",
      "Epoch 6: val_loss did not improve from 0.62386\n",
      "334/334 [==============================] - 1s 4ms/step - loss: 0.0797 - accuracy: 0.9780 - val_loss: 0.6368 - val_accuracy: 0.8180\n",
      "Epoch 7/20\n",
      "331/334 [============================>.] - ETA: 0s - loss: 0.0564 - accuracy: 0.9852\n",
      "Epoch 7: val_loss did not improve from 0.62386\n",
      "334/334 [==============================] - 1s 4ms/step - loss: 0.0562 - accuracy: 0.9853 - val_loss: 0.6465 - val_accuracy: 0.8084\n",
      "Epoch 8/20\n",
      "320/334 [===========================>..] - ETA: 0s - loss: 0.0398 - accuracy: 0.9892\n",
      "Epoch 8: val_loss did not improve from 0.62386\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 0.0402 - accuracy: 0.9894 - val_loss: 0.6474 - val_accuracy: 0.8287\n",
      "Epoch 9/20\n",
      "334/334 [==============================] - ETA: 0s - loss: 0.0285 - accuracy: 0.9946\n",
      "Epoch 9: val_loss did not improve from 0.62386\n",
      "334/334 [==============================] - 1s 4ms/step - loss: 0.0285 - accuracy: 0.9946 - val_loss: 0.6438 - val_accuracy: 0.8240\n",
      "Epoch 10/20\n",
      "322/334 [===========================>..] - ETA: 0s - loss: 0.0210 - accuracy: 0.9953\n",
      "Epoch 10: val_loss did not improve from 0.62386\n",
      "334/334 [==============================] - 1s 4ms/step - loss: 0.0221 - accuracy: 0.9951 - val_loss: 0.6913 - val_accuracy: 0.8168\n",
      "Epoch 11/20\n",
      "334/334 [==============================] - ETA: 0s - loss: 0.0175 - accuracy: 0.9960\n",
      "Epoch 11: val_loss did not improve from 0.62386\n",
      "334/334 [==============================] - 1s 4ms/step - loss: 0.0175 - accuracy: 0.9960 - val_loss: 0.6713 - val_accuracy: 0.8240\n",
      "Epoch 12/20\n",
      "332/334 [============================>.] - ETA: 0s - loss: 0.0126 - accuracy: 0.9971\n",
      "Epoch 12: val_loss did not improve from 0.62386\n",
      "334/334 [==============================] - 1s 4ms/step - loss: 0.0126 - accuracy: 0.9972 - val_loss: 0.6748 - val_accuracy: 0.8287\n",
      "Epoch 13/20\n",
      "324/334 [============================>.] - ETA: 0s - loss: 0.0099 - accuracy: 0.9980\n",
      "Epoch 13: val_loss did not improve from 0.62386\n",
      "334/334 [==============================] - 1s 4ms/step - loss: 0.0100 - accuracy: 0.9979 - val_loss: 0.6447 - val_accuracy: 0.8443\n",
      "Epoch 14/20\n",
      "329/334 [============================>.] - ETA: 0s - loss: 0.0076 - accuracy: 0.9983\n",
      "Epoch 14: val_loss did not improve from 0.62386\n",
      "334/334 [==============================] - 1s 4ms/step - loss: 0.0076 - accuracy: 0.9984 - val_loss: 0.6631 - val_accuracy: 0.8323\n",
      "Epoch 15/20\n",
      "325/334 [============================>.] - ETA: 0s - loss: 0.0061 - accuracy: 0.9983\n",
      "Epoch 15: val_loss did not improve from 0.62386\n",
      "334/334 [==============================] - 1s 4ms/step - loss: 0.0061 - accuracy: 0.9984 - val_loss: 0.6869 - val_accuracy: 0.8335\n",
      "Epoch 16/20\n",
      "333/334 [============================>.] - ETA: 0s - loss: 0.0068 - accuracy: 0.9985\n",
      "Epoch 16: val_loss did not improve from 0.62386\n",
      "334/334 [==============================] - 1s 4ms/step - loss: 0.0067 - accuracy: 0.9985 - val_loss: 0.6576 - val_accuracy: 0.8443\n",
      "Epoch 17/20\n",
      "329/334 [============================>.] - ETA: 0s - loss: 0.0056 - accuracy: 0.9988\n",
      "Epoch 17: val_loss did not improve from 0.62386\n",
      "334/334 [==============================] - 1s 4ms/step - loss: 0.0056 - accuracy: 0.9988 - val_loss: 0.6814 - val_accuracy: 0.8407\n",
      "Epoch 18/20\n",
      "316/334 [===========================>..] - ETA: 0s - loss: 0.0054 - accuracy: 0.9992\n",
      "Epoch 18: val_loss did not improve from 0.62386\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 0.0053 - accuracy: 0.9991 - val_loss: 0.6596 - val_accuracy: 0.8407\n",
      "Epoch 19/20\n",
      "316/334 [===========================>..] - ETA: 0s - loss: 0.0048 - accuracy: 0.9986\n",
      "Epoch 19: val_loss did not improve from 0.62386\n",
      "334/334 [==============================] - 1s 4ms/step - loss: 0.0048 - accuracy: 0.9985 - val_loss: 0.6794 - val_accuracy: 0.8335\n",
      "Epoch 20/20\n",
      "322/334 [===========================>..] - ETA: 0s - loss: 0.0034 - accuracy: 0.9989\n",
      "Epoch 20: val_loss did not improve from 0.62386\n",
      "334/334 [==============================] - 1s 4ms/step - loss: 0.0038 - accuracy: 0.9988 - val_loss: 0.6830 - val_accuracy: 0.8527\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x174142446a0>"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "Resnet50_model.load_weights('saved_models/weights.best.Resnet50.hdf5')"
   ],
   "metadata": {
    "id": "nBOjQNldmxi6"
   },
   "execution_count": 63,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\n",
    "print(f\"Accuracy: {accuracy_score(Y_test, np_utils.to_categorical(Resnet50_predictions, 133))*100}%\")\n",
    "print(f\"Recall: {recall_score(Y_test, np_utils.to_categorical(Resnet50_predictions, 133), average='weighted', zero_division=1)*100}%\")\n",
    "print(f\"Precision: {precision_score(Y_test, np_utils.to_categorical(Resnet50_predictions, 133), average='weighted', zero_division=1)*100}%\")\n",
    "print(f\"F1 score: {f1_score(Y_test, np_utils.to_categorical(Resnet50_predictions, 133), average='weighted', zero_division=1)*100}%\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1nUaxnmnnB-7",
    "outputId": "f143c561-2e0a-4809-f037-f13c027dc4e2"
   },
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Accuracy: 82.17703349282297%\n",
      "Recall: 82.17703349282297%\n",
      "Precision: 85.54294555789771%\n",
      "F1 score: 81.90072326923996%\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Разбира се, резултатите на тестовите данни са доста по-добри с претренирания модел."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "results = [\n",
    "    [5, 30.91],\n",
    "    [10, 12.22],\n",
    "    [133, 81.90]\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(results, columns=['Number of breeds', 'F1 score'])\n",
    "df"
   ],
   "metadata": {
    "id": "c3dtOLLYpaHr"
   },
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "   Number of breeds  F1 score\n0                 5     30.91\n1                10     12.22\n2               133     81.90",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Number of breeds</th>\n      <th>F1 score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5</td>\n      <td>30.91</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10</td>\n      <td>12.22</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>133</td>\n      <td>81.90</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 600x400 with 0 Axes>"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "Text(0.5, 0, 'Number of  breeds')"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "Text(0, 0.5, 'F1 score')"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x1741dd58940>]"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 600x400 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(dpi=100)\n",
    "plt.xlabel(\"Number of  breeds\", fontweight=\"bold\")\n",
    "plt.ylabel(\"F1 score\", fontweight=\"bold\")\n",
    "plt.plot(df['Number of breeds'], df['F1 score'], color=\"blue\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "И за финал, диаграмата показва разликата между f1 score-овете на разгледаните модели."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
